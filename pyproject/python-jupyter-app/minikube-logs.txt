
==> Audit <==
|----------------|--------------------------------|----------|-------------------------------|---------|---------------------|---------------------|
|    Command     |              Args              | Profile  |             User              | Version |     Start Time      |      End Time       |
|----------------|--------------------------------|----------|-------------------------------|---------|---------------------|---------------------|
| start          |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 07 Apr 25 19:58 IST | 07 Apr 25 20:02 IST |
| start          |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 08 Apr 25 11:07 IST |                     |
| start          |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 08 Apr 25 11:08 IST |                     |
| start          |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 19:50 IST | 14 Apr 25 19:52 IST |
| image          | load myapp                     | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 19:52 IST |                     |
| image          | load jupyter-notebook-app      | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 19:52 IST | 14 Apr 25 19:53 IST |
| docker-env     |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 19:58 IST | 14 Apr 25 19:58 IST |
| ip             |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:00 IST |                     |
| service        | myapp-service                  | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:02 IST |                     |
| stop           |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:04 IST |                     |
| docker-env     | minikube docker-env            | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:15 IST |                     |
| docker-env     | minikube docker-env            | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:16 IST |                     |
| start          |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:35 IST | 14 Apr 25 20:36 IST |
| docker-env     | minikube docker-env            | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:37 IST | 14 Apr 25 20:37 IST |
| service        | python-jupyter-deployment      | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:43 IST |                     |
|                | --url                          |          |                               |         |                     |                     |
| docker-env     |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:49 IST | 14 Apr 25 20:49 IST |
| docker-env     | minikube docker-env            | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:49 IST | 14 Apr 25 20:49 IST |
| update-context |                                | minikube | B7E524E56C995EA\Administrator | v1.35.0 | 14 Apr 25 20:58 IST | 14 Apr 25 20:58 IST |
|----------------|--------------------------------|----------|-------------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/04/14 20:35:25
Running on machine: b7e524e56c995ea
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0414 20:35:25.494865   13164 out.go:345] Setting OutFile to fd 92 ...
I0414 20:35:25.497408   13164 out.go:397] isatty.IsTerminal(92) = true
I0414 20:35:25.497408   13164 out.go:358] Setting ErrFile to fd 96...
I0414 20:35:25.497902   13164 out.go:397] isatty.IsTerminal(96) = true
W0414 20:35:25.518999   13164 root.go:314] Error reading config file at C:\Users\Administrator\.minikube\config\config.json: open C:\Users\Administrator\.minikube\config\config.json: The system cannot find the file specified.
I0414 20:35:25.527649   13164 out.go:352] Setting JSON to false
I0414 20:35:25.531615   13164 start.go:129] hostinfo: {"hostname":"b7e524e56c995ea","uptime":16534,"bootTime":1744626590,"procs":232,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5487 Build 19045.5487","kernelVersion":"10.0.19045.5487 Build 19045.5487","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"eaad01dd-4ce6-45e0-91b4-c992a1351d0a"}
W0414 20:35:25.531615   13164 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0414 20:35:25.540285   13164 out.go:177] üòÑ  minikube v1.35.0 on Microsoft Windows 10 Pro 10.0.19045.5487 Build 19045.5487
I0414 20:35:25.551651   13164 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0414 20:35:25.552477   13164 notify.go:220] Checking for updates...
I0414 20:35:25.555192   13164 driver.go:394] Setting default libvirt URI to qemu:///system
I0414 20:35:26.071425   13164 docker.go:123] docker version: linux-28.0.1:Docker Desktop 4.39.0 (184744)
I0414 20:35:26.088042   13164 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0414 20:35:28.707112   13164 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.6190692s)
I0414 20:35:28.714114   13164 info.go:266] docker info: {ID:f0e9f42b-3a5d-4b24-9869-3734d259bf79 Containers:9 ContainersRunning:2 ContainersPaused:0 ContainersStopped:7 Images:7 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:95 OomKillDisable:true NGoroutines:111 SystemTime:2025-04-14 15:05:28.427023721 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4109230080 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v0.9.4] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.21.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.33.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.5] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.3]] Warnings:<nil>}}
I0414 20:35:28.716148   13164 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0414 20:35:28.719112   13164 start.go:297] selected driver: docker
I0414 20:35:28.719112   13164 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0414 20:35:28.719112   13164 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0414 20:35:28.747353   13164 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0414 20:35:29.090783   13164 info.go:266] docker info: {ID:f0e9f42b-3a5d-4b24-9869-3734d259bf79 Containers:9 ContainersRunning:2 ContainersPaused:0 ContainersStopped:7 Images:7 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:94 OomKillDisable:true NGoroutines:109 SystemTime:2025-04-14 15:05:29.062469587 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:18 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4109230080 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.2.4-0-g6c52b3f Expected:v1.2.4-0-g6c52b3f} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v0.9.4] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.21.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.33.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.5] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.3]] Warnings:<nil>}}
I0414 20:35:29.155631   13164 cni.go:84] Creating CNI manager for ""
I0414 20:35:29.158732   13164 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0414 20:35:29.158732   13164 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0414 20:35:29.158732   13164 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0414 20:35:29.163974   13164 cache.go:121] Beginning downloading kic base image for docker with docker
I0414 20:35:29.163974   13164 out.go:177] üöú  Pulling base image v0.0.46 ...
I0414 20:35:29.163974   13164 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0414 20:35:29.163974   13164 preload.go:146] Found local preload: C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0414 20:35:29.163974   13164 cache.go:56] Caching tarball of preloaded images
I0414 20:35:29.163974   13164 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0414 20:35:29.163974   13164 preload.go:172] Found C:\Users\Administrator\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0414 20:35:29.163974   13164 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0414 20:35:29.163974   13164 profile.go:143] Saving config to C:\Users\Administrator\.minikube\profiles\minikube\config.json ...
I0414 20:35:29.447323   13164 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0414 20:35:29.450042   13164 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0414 20:35:29.450042   13164 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0414 20:35:29.451278   13164 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0414 20:35:29.451303   13164 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0414 20:35:29.451303   13164 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0414 20:35:29.451405   13164 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0414 20:35:29.451405   13164 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0414 20:35:29.451405   13164 localpath.go:146] windows sanitize: C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\Administrator\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0414 20:36:04.461202   13164 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0414 20:36:04.464194   13164 cache.go:227] Successfully downloaded all kic artifacts
I0414 20:36:04.474589   13164 start.go:360] acquireMachinesLock for minikube: {Name:mk3f259f80712de9f83f91ad0f12658ee47ef5bf Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0414 20:36:04.474589   13164 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0414 20:36:04.475182   13164 start.go:96] Skipping create...Using existing machine configuration
I0414 20:36:04.475726   13164 fix.go:54] fixHost starting: 
I0414 20:36:04.504197   13164 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 20:36:04.623105   13164 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0414 20:36:04.623105   13164 fix.go:138] unexpected machine state, will restart: <nil>
I0414 20:36:04.624996   13164 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0414 20:36:04.626759   13164 machine.go:93] provisionDockerMachine start ...
I0414 20:36:04.644128   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:04.760786   13164 main.go:141] libmachine: Using SSH client type: native
I0414 20:36:04.770631   13164 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1435360] 0x1437ea0 <nil>  [] 0s} 127.0.0.1 58243 <nil> <nil>}
I0414 20:36:04.770631   13164 main.go:141] libmachine: About to run SSH command:
hostname
I0414 20:36:05.188005   13164 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0414 20:36:05.188571   13164 ubuntu.go:169] provisioning hostname "minikube"
I0414 20:36:05.202780   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:05.269231   13164 main.go:141] libmachine: Using SSH client type: native
I0414 20:36:05.269369   13164 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1435360] 0x1437ea0 <nil>  [] 0s} 127.0.0.1 58243 <nil> <nil>}
I0414 20:36:05.269369   13164 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0414 20:36:05.530778   13164 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0414 20:36:05.546676   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:05.613092   13164 main.go:141] libmachine: Using SSH client type: native
I0414 20:36:05.613092   13164 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1435360] 0x1437ea0 <nil>  [] 0s} 127.0.0.1 58243 <nil> <nil>}
I0414 20:36:05.613092   13164 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0414 20:36:05.809871   13164 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0414 20:36:05.811128   13164 ubuntu.go:175] set auth options {CertDir:C:\Users\Administrator\.minikube CaCertPath:C:\Users\Administrator\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Administrator\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Administrator\.minikube\machines\server.pem ServerKeyPath:C:\Users\Administrator\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Administrator\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Administrator\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Administrator\.minikube}
I0414 20:36:05.811128   13164 ubuntu.go:177] setting up certificates
I0414 20:36:05.811128   13164 provision.go:84] configureAuth start
I0414 20:36:05.828056   13164 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0414 20:36:05.891272   13164 provision.go:143] copyHostCerts
I0414 20:36:05.895865   13164 exec_runner.go:144] found C:\Users\Administrator\.minikube/cert.pem, removing ...
I0414 20:36:05.896419   13164 exec_runner.go:203] rm: C:\Users\Administrator\.minikube\cert.pem
I0414 20:36:05.897159   13164 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\cert.pem --> C:\Users\Administrator\.minikube/cert.pem (1139 bytes)
I0414 20:36:05.901426   13164 exec_runner.go:144] found C:\Users\Administrator\.minikube/key.pem, removing ...
I0414 20:36:05.901426   13164 exec_runner.go:203] rm: C:\Users\Administrator\.minikube\key.pem
I0414 20:36:05.901426   13164 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\key.pem --> C:\Users\Administrator\.minikube/key.pem (1675 bytes)
I0414 20:36:05.903740   13164 exec_runner.go:144] found C:\Users\Administrator\.minikube/ca.pem, removing ...
I0414 20:36:05.903740   13164 exec_runner.go:203] rm: C:\Users\Administrator\.minikube\ca.pem
I0414 20:36:05.903740   13164 exec_runner.go:151] cp: C:\Users\Administrator\.minikube\certs\ca.pem --> C:\Users\Administrator\.minikube/ca.pem (1094 bytes)
I0414 20:36:05.906383   13164 provision.go:117] generating server cert: C:\Users\Administrator\.minikube\machines\server.pem ca-key=C:\Users\Administrator\.minikube\certs\ca.pem private-key=C:\Users\Administrator\.minikube\certs\ca-key.pem org=Administrator.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0414 20:36:06.040894   13164 provision.go:177] copyRemoteCerts
I0414 20:36:06.062776   13164 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0414 20:36:06.075916   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:06.129576   13164 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58243 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0414 20:36:06.281330   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1094 bytes)
I0414 20:36:06.345434   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I0414 20:36:06.388650   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0414 20:36:06.441615   13164 provision.go:87] duration metric: took 628.8541ms to configureAuth
I0414 20:36:06.441615   13164 ubuntu.go:193] setting minikube options for container-runtime
I0414 20:36:06.441937   13164 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0414 20:36:06.453514   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:06.514855   13164 main.go:141] libmachine: Using SSH client type: native
I0414 20:36:06.515376   13164 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1435360] 0x1437ea0 <nil>  [] 0s} 127.0.0.1 58243 <nil> <nil>}
I0414 20:36:06.515376   13164 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0414 20:36:06.727958   13164 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0414 20:36:06.727958   13164 ubuntu.go:71] root file system type: overlay
I0414 20:36:06.727958   13164 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0414 20:36:06.740468   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:06.810513   13164 main.go:141] libmachine: Using SSH client type: native
I0414 20:36:06.811089   13164 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1435360] 0x1437ea0 <nil>  [] 0s} 127.0.0.1 58243 <nil> <nil>}
I0414 20:36:06.811089   13164 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0414 20:36:07.028353   13164 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0414 20:36:07.042983   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:07.100731   13164 main.go:141] libmachine: Using SSH client type: native
I0414 20:36:07.100731   13164 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1435360] 0x1437ea0 <nil>  [] 0s} 127.0.0.1 58243 <nil> <nil>}
I0414 20:36:07.100731   13164 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0414 20:36:07.314844   13164 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0414 20:36:07.314844   13164 machine.go:96] duration metric: took 2.6880846s to provisionDockerMachine
I0414 20:36:07.316294   13164 start.go:293] postStartSetup for "minikube" (driver="docker")
I0414 20:36:07.316294   13164 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0414 20:36:07.337324   13164 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0414 20:36:07.349209   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:07.415111   13164 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58243 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0414 20:36:07.561186   13164 ssh_runner.go:195] Run: cat /etc/os-release
I0414 20:36:07.574669   13164 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0414 20:36:07.574669   13164 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0414 20:36:07.574669   13164 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0414 20:36:07.574669   13164 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0414 20:36:07.574669   13164 filesync.go:126] Scanning C:\Users\Administrator\.minikube\addons for local assets ...
I0414 20:36:07.576453   13164 filesync.go:126] Scanning C:\Users\Administrator\.minikube\files for local assets ...
I0414 20:36:07.576796   13164 start.go:296] duration metric: took 260.5016ms for postStartSetup
I0414 20:36:07.598208   13164 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0414 20:36:07.603916   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:07.669149   13164 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58243 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0414 20:36:07.842280   13164 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0414 20:36:07.858864   13164 fix.go:56] duration metric: took 3.3831387s for fixHost
I0414 20:36:07.858864   13164 start.go:83] releasing machines lock for "minikube", held for 3.3842751s
I0414 20:36:07.873911   13164 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0414 20:36:07.928693   13164 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0414 20:36:07.943794   13164 ssh_runner.go:195] Run: cat /version.json
I0414 20:36:07.943794   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:07.956369   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:08.051953   13164 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58243 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0414 20:36:08.053252   13164 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58243 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
W0414 20:36:08.186712   13164 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0414 20:36:08.210739   13164 ssh_runner.go:195] Run: systemctl --version
I0414 20:36:08.261118   13164 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0414 20:36:08.293566   13164 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0414 20:36:08.323875   13164 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0414 20:36:08.341458   13164 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0414 20:36:08.362282   13164 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0414 20:36:08.362282   13164 start.go:495] detecting cgroup driver to use...
I0414 20:36:08.362282   13164 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0414 20:36:08.367268   13164 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0414 20:36:08.419752   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0414 20:36:08.472854   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0414 20:36:08.494930   13164 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0414 20:36:08.516160   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
W0414 20:36:08.533028   13164 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0414 20:36:08.533657   13164 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0414 20:36:08.582503   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0414 20:36:08.661534   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0414 20:36:08.708027   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0414 20:36:08.746650   13164 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0414 20:36:08.851936   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0414 20:36:08.887121   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0414 20:36:08.925563   13164 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0414 20:36:08.968411   13164 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0414 20:36:09.009335   13164 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0414 20:36:09.045697   13164 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 20:36:09.757150   13164 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0414 20:36:21.374262   13164 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (11.6171126s)
I0414 20:36:21.374262   13164 start.go:495] detecting cgroup driver to use...
I0414 20:36:21.374262   13164 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0414 20:36:21.395054   13164 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0414 20:36:21.424167   13164 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0414 20:36:21.436476   13164 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0414 20:36:21.467038   13164 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0414 20:36:21.548313   13164 ssh_runner.go:195] Run: which cri-dockerd
I0414 20:36:21.607699   13164 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0414 20:36:21.670018   13164 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0414 20:36:21.740844   13164 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0414 20:36:21.981088   13164 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0414 20:36:22.153068   13164 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0414 20:36:22.159869   13164 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0414 20:36:22.220820   13164 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 20:36:22.381580   13164 ssh_runner.go:195] Run: sudo systemctl restart docker
I0414 20:36:28.380240   13164 ssh_runner.go:235] Completed: sudo systemctl restart docker: (5.9986605s)
I0414 20:36:28.397659   13164 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0414 20:36:28.440372   13164 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0414 20:36:28.506764   13164 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0414 20:36:28.549024   13164 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0414 20:36:28.747550   13164 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0414 20:36:28.947671   13164 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 20:36:29.144019   13164 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0414 20:36:29.195571   13164 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0414 20:36:29.237058   13164 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 20:36:29.428927   13164 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0414 20:36:30.336082   13164 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0414 20:36:30.356347   13164 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0414 20:36:30.369107   13164 start.go:563] Will wait 60s for crictl version
I0414 20:36:30.387199   13164 ssh_runner.go:195] Run: which crictl
I0414 20:36:30.416058   13164 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0414 20:36:31.217013   13164 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0414 20:36:31.235730   13164 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0414 20:36:32.546427   13164 ssh_runner.go:235] Completed: docker version --format {{.Server.Version}}: (1.3106976s)
I0414 20:36:32.559597   13164 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0414 20:36:32.848056   13164 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0414 20:36:32.861694   13164 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0414 20:36:35.381435   13164 cli_runner.go:217] Completed: docker exec -t minikube dig +short host.docker.internal: (2.5181869s)
I0414 20:36:35.381435   13164 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0414 20:36:35.434725   13164 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0414 20:36:35.533764   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0414 20:36:35.625609   13164 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0414 20:36:35.627629   13164 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0414 20:36:35.682262   13164 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0414 20:36:35.828676   13164 docker.go:689] Got preloaded images: -- stdout --
jupyter-notebook-app:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0414 20:36:35.828676   13164 docker.go:619] Images already preloaded, skipping extraction
I0414 20:36:35.848145   13164 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0414 20:36:36.131312   13164 docker.go:689] Got preloaded images: -- stdout --
jupyter-notebook-app:latest
nginx:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0414 20:36:36.138965   13164 cache_images.go:84] Images are preloaded, skipping loading
I0414 20:36:36.141235   13164 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0414 20:36:36.215394   13164 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0414 20:36:36.229191   13164 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0414 20:36:40.613039   13164 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (4.3838481s)
I0414 20:36:40.613039   13164 cni.go:84] Creating CNI manager for ""
I0414 20:36:40.613039   13164 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0414 20:36:40.614267   13164 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0414 20:36:40.614495   13164 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0414 20:36:40.615135   13164 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0414 20:36:40.632397   13164 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0414 20:36:40.757769   13164 binaries.go:44] Found k8s binaries, skipping transfer
I0414 20:36:40.807214   13164 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0414 20:36:40.840825   13164 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0414 20:36:41.014244   13164 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0414 20:36:41.242570   13164 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0414 20:36:41.466276   13164 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0414 20:36:41.571696   13164 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 20:36:42.140301   13164 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0414 20:36:42.240398   13164 certs.go:68] Setting up C:\Users\Administrator\.minikube\profiles\minikube for IP: 192.168.49.2
I0414 20:36:42.240398   13164 certs.go:194] generating shared ca certs ...
I0414 20:36:42.240398   13164 certs.go:226] acquiring lock for ca certs: {Name:mk38b1aba3ca636999c633fc59ac7bca24a1dea0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 20:36:42.242734   13164 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Administrator\.minikube\ca.key
I0414 20:36:42.243373   13164 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Administrator\.minikube\proxy-client-ca.key
I0414 20:36:42.243373   13164 certs.go:256] generating profile certs ...
I0414 20:36:42.247155   13164 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\Administrator\.minikube\profiles\minikube\client.key
I0414 20:36:42.247155   13164 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0414 20:36:42.252156   13164 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key
I0414 20:36:42.254140   13164 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\ca-key.pem (1679 bytes)
I0414 20:36:42.254140   13164 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\ca.pem (1094 bytes)
I0414 20:36:42.254140   13164 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\cert.pem (1139 bytes)
I0414 20:36:42.254140   13164 certs.go:484] found cert: C:\Users\Administrator\.minikube\certs\key.pem (1675 bytes)
I0414 20:36:42.303228   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0414 20:36:42.430569   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0414 20:36:42.640949   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0414 20:36:42.716996   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0414 20:36:42.926292   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0414 20:36:43.142369   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0414 20:36:43.233557   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0414 20:36:43.334189   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0414 20:36:43.440040   13164 ssh_runner.go:362] scp C:\Users\Administrator\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0414 20:36:43.649812   13164 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0414 20:36:43.925946   13164 ssh_runner.go:195] Run: openssl version
I0414 20:36:44.209155   13164 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0414 20:36:44.282137   13164 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0414 20:36:44.359514   13164 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Apr  7 14:32 /usr/share/ca-certificates/minikubeCA.pem
I0414 20:36:44.388792   13164 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0414 20:36:44.690678   13164 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0414 20:36:44.897740   13164 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0414 20:36:44.954694   13164 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0414 20:36:45.065984   13164 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0414 20:36:45.163055   13164 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0414 20:36:45.238138   13164 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0414 20:36:45.355194   13164 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0414 20:36:45.442715   13164 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0414 20:36:45.485759   13164 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrator:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0414 20:36:45.501275   13164 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0414 20:36:45.687038   13164 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0414 20:36:45.736967   13164 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0414 20:36:45.737769   13164 kubeadm.go:593] restartPrimaryControlPlane start ...
I0414 20:36:45.755173   13164 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0414 20:36:45.815503   13164 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0414 20:36:45.876700   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0414 20:36:46.012679   13164 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:58242"
I0414 20:36:46.199217   13164 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0414 20:36:46.269597   13164 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0414 20:36:46.270598   13164 kubeadm.go:597] duration metric: took 532.8286ms to restartPrimaryControlPlane
I0414 20:36:46.270598   13164 kubeadm.go:394] duration metric: took 786.8289ms to StartCluster
I0414 20:36:46.273096   13164 settings.go:142] acquiring lock: {Name:mk1dc43883b192668fc34902159f52a719564aae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 20:36:46.273848   13164 settings.go:150] Updating kubeconfig:  C:\Users\Administrator\.kube\config
I0414 20:36:46.288350   13164 lock.go:35] WriteFile acquiring C:\Users\Administrator\.kube\config: {Name:mkea657c15dc45de69286aed2fa89967aee013e4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0414 20:36:46.291176   13164 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0414 20:36:46.291771   13164 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0414 20:36:46.295440   13164 out.go:177] üîé  Verifying Kubernetes components...
I0414 20:36:46.301459   13164 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0414 20:36:46.310987   13164 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0414 20:36:46.310987   13164 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0414 20:36:46.311995   13164 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0414 20:36:46.311995   13164 addons.go:247] addon storage-provisioner should already be in state true
I0414 20:36:46.314660   13164 host.go:66] Checking if "minikube" exists ...
I0414 20:36:46.314660   13164 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0414 20:36:46.377538   13164 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0414 20:36:46.449967   13164 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 20:36:46.455818   13164 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 20:36:46.534729   13164 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0414 20:36:46.535720   13164 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0414 20:36:46.535720   13164 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0414 20:36:46.551923   13164 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0414 20:36:46.551923   13164 addons.go:247] addon default-storageclass should already be in state true
I0414 20:36:46.551923   13164 host.go:66] Checking if "minikube" exists ...
I0414 20:36:46.552920   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:46.628375   13164 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0414 20:36:46.689522   13164 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58243 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0414 20:36:46.715375   13164 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0414 20:36:46.715375   13164 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0414 20:36:46.726857   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0414 20:36:46.791302   13164 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:58243 SSHKeyPath:C:\Users\Administrator\.minikube\machines\minikube\id_rsa Username:docker}
I0414 20:36:46.801665   13164 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0414 20:36:46.838845   13164 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0414 20:36:46.892265   13164 api_server.go:52] waiting for apiserver process to appear ...
I0414 20:36:46.910618   13164 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0414 20:36:46.965680   13164 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0414 20:36:46.983697   13164 api_server.go:72] duration metric: took 691.9263ms to wait for apiserver process to appear ...
I0414 20:36:46.983697   13164 api_server.go:88] waiting for apiserver healthz status ...
I0414 20:36:46.983697   13164 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58242/healthz ...
I0414 20:36:47.009908   13164 api_server.go:279] https://127.0.0.1:58242/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0414 20:36:47.009908   13164 api_server.go:103] status: https://127.0.0.1:58242/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0414 20:36:47.032407   13164 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0414 20:36:47.484179   13164 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:58242/healthz ...
I0414 20:36:47.517761   13164 api_server.go:279] https://127.0.0.1:58242/healthz returned 200:
ok
I0414 20:36:47.544839   13164 api_server.go:141] control plane version: v1.32.0
I0414 20:36:47.544839   13164 api_server.go:131] duration metric: took 561.1417ms to wait for apiserver health ...
I0414 20:36:47.544839   13164 system_pods.go:43] waiting for kube-system pods to appear ...
I0414 20:36:47.614081   13164 system_pods.go:59] 7 kube-system pods found
I0414 20:36:47.614081   13164 system_pods.go:61] "coredns-668d6bf9bc-8fttd" [4829e0a2-e1a7-410f-a453-ec80829397f7] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0414 20:36:47.614081   13164 system_pods.go:61] "etcd-minikube" [bd0b918a-46b1-4d1a-bec7-87e4a9382cbd] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0414 20:36:47.614081   13164 system_pods.go:61] "kube-apiserver-minikube" [63a3ee1c-6dde-424d-9dc4-984b4e466503] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0414 20:36:47.614081   13164 system_pods.go:61] "kube-controller-manager-minikube" [f7911add-1cfe-461d-b393-b3c9747a80bc] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0414 20:36:47.614081   13164 system_pods.go:61] "kube-proxy-fxbgl" [65b97594-4415-4b2a-9da4-e0a515a9df7e] Running
I0414 20:36:47.614081   13164 system_pods.go:61] "kube-scheduler-minikube" [4f3373dd-7714-483f-b0e6-18b226254857] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0414 20:36:47.614081   13164 system_pods.go:61] "storage-provisioner" [4934d07a-c19b-4a74-aeae-6230a125c3fe] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0414 20:36:47.614081   13164 system_pods.go:74] duration metric: took 69.2423ms to wait for pod list to return data ...
I0414 20:36:47.614081   13164 kubeadm.go:582] duration metric: took 1.3223103s to wait for: map[apiserver:true system_pods:true]
I0414 20:36:47.614081   13164 node_conditions.go:102] verifying NodePressure condition ...
I0414 20:36:47.635605   13164 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0414 20:36:47.635605   13164 node_conditions.go:123] node cpu capacity is 4
I0414 20:36:47.636999   13164 node_conditions.go:105] duration metric: took 22.803ms to run NodePressure ...
I0414 20:36:47.637099   13164 start.go:241] waiting for startup goroutines ...
I0414 20:36:49.464379   13164 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.4319723s)
I0414 20:36:49.464379   13164 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.4986988s)
I0414 20:36:49.490719   13164 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0414 20:36:49.492263   13164 addons.go:514] duration metric: took 3.2010873s for enable addons: enabled=[storage-provisioner default-storageclass]
I0414 20:36:49.492307   13164 start.go:246] waiting for cluster config update ...
I0414 20:36:49.492307   13164 start.go:255] writing updated cluster config ...
I0414 20:36:49.510141   13164 ssh_runner.go:195] Run: rm -f paused
I0414 20:36:49.666689   13164 start.go:600] kubectl: 1.32.2, cluster: 1.32.0 (minor skew: 0)
I0414 20:36:49.671607   13164 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Apr 14 15:06:30 minikube cri-dockerd[13922]: time="2025-04-14T15:06:30Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Apr 14 15:06:30 minikube cri-dockerd[13922]: time="2025-04-14T15:06:30Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Apr 14 15:06:30 minikube cri-dockerd[13922]: time="2025-04-14T15:06:30Z" level=info msg="Start cri-dockerd grpc backend"
Apr 14 15:06:30 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Apr 14 15:06:30 minikube cri-dockerd[13922]: time="2025-04-14T15:06:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"myapp-deployment-7df474b884-4zz57_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1b346d064d1d819b151fbb291a1dd93057ce8e4375277f0db97522eb11f8291c\""
Apr 14 15:06:30 minikube cri-dockerd[13922]: time="2025-04-14T15:06:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-8fttd_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9b490d8036590b98f9d00d4d8f1226dd6f8eca7a8bd4897ad4d8dac619eac289\""
Apr 14 15:06:30 minikube cri-dockerd[13922]: time="2025-04-14T15:06:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-8fttd_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"989fa1b5239656c8c6bb8d130035a259636ce4427a9b4abe38a9fb0ba9271691\""
Apr 14 15:06:30 minikube cri-dockerd[13922]: time="2025-04-14T15:06:30Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"myapp-deployment-7448ddcf95-tvq2s_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e051e7dfaf96ad0f38e1dbcb4bff074e618483e7d55c897367900289011b9177\""
Apr 14 15:06:31 minikube cri-dockerd[13922]: time="2025-04-14T15:06:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1ba1bbe836e1de4a9531bec1bcd11f4917c5c230a6421f83464fdf1836e5b62d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 15:06:32 minikube cri-dockerd[13922]: time="2025-04-14T15:06:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/45c3327e1fc5269589eb1f356c10886935693c03b3835b121aadfe547a5c8db9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 15:06:32 minikube cri-dockerd[13922]: time="2025-04-14T15:06:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/53c38397730b25ffda2b0dd0188c4b7813c7f2d9345c75c5eae75f94d1abbce1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 15:06:32 minikube cri-dockerd[13922]: time="2025-04-14T15:06:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6116ed8f7f03a4b67150892f4989f4af2742b10dd4262d48894840ff2eb4e07b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 15:06:32 minikube cri-dockerd[13922]: time="2025-04-14T15:06:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3d6035840eba18da0caa8598b3fd4fa13f9d101ea3b841c55517c57484ac8b25/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 15:06:33 minikube cri-dockerd[13922]: time="2025-04-14T15:06:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b98e03d811ae64a5ce352135af78fb4d2f8a6588283ea42113a65f0a0b1fb2ca/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 15:06:33 minikube cri-dockerd[13922]: time="2025-04-14T15:06:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5df7745da68c9cf4a5cd3936aa5054780016a62a2754c986969677382d242b25/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 15:06:34 minikube cri-dockerd[13922]: time="2025-04-14T15:06:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1464eb2f771187b7debd90e9a4306a43377f69a23220d3831da22c157069c639/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 15:06:35 minikube cri-dockerd[13922]: time="2025-04-14T15:06:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1818c0c8ffccd6fd51ca904e114d1e23fa73b998036f62b7be46947e6d85822b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Apr 14 15:06:36 minikube dockerd[13601]: time="2025-04-14T15:06:36.603288346Z" level=info msg="ignoring event" container=5d498173d4f188768946048831229d305eaed11d268ae383040bf56caefb6c2b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 15:09:43 minikube dockerd[13601]: time="2025-04-14T15:09:43.245620028Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:09:43 minikube dockerd[13601]: time="2025-04-14T15:09:43.245894261Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:10:22 minikube dockerd[13601]: time="2025-04-14T15:10:22.890484733Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:10:22 minikube dockerd[13601]: time="2025-04-14T15:10:22.890542840Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:12:55 minikube cri-dockerd[13922]: time="2025-04-14T15:12:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e5dfa6b3e0da8b78f1276ca73bb7511798b066206c435db41e27bac05f4bb276/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 15:12:59 minikube dockerd[13601]: time="2025-04-14T15:12:59.904751700Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:12:59 minikube dockerd[13601]: time="2025-04-14T15:12:59.905112845Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:13:21 minikube dockerd[13601]: time="2025-04-14T15:13:21.907290310Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:13:21 minikube dockerd[13601]: time="2025-04-14T15:13:21.907626560Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:13:50 minikube dockerd[13601]: time="2025-04-14T15:13:50.930550567Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:13:50 minikube dockerd[13601]: time="2025-04-14T15:13:50.930952551Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:14:35 minikube dockerd[13601]: time="2025-04-14T15:14:35.855454457Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:14:35 minikube dockerd[13601]: time="2025-04-14T15:14:35.855851826Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:14:46 minikube dockerd[13601]: time="2025-04-14T15:14:46.856874567Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:14:46 minikube dockerd[13601]: time="2025-04-14T15:14:46.857081754Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:15:33 minikube dockerd[13601]: time="2025-04-14T15:15:33.708991098Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:15:33 minikube dockerd[13601]: time="2025-04-14T15:15:33.709136355Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:16:14 minikube dockerd[13601]: time="2025-04-14T15:16:14.404995575Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:16:14 minikube dockerd[13601]: time="2025-04-14T15:16:14.405544097Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:19:05 minikube dockerd[13601]: time="2025-04-14T15:19:05.134561802Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:19:05 minikube dockerd[13601]: time="2025-04-14T15:19:05.135150217Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:19:52 minikube dockerd[13601]: time="2025-04-14T15:19:52.320653444Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:19:52 minikube dockerd[13601]: time="2025-04-14T15:19:52.321260278Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:20:11 minikube dockerd[13601]: time="2025-04-14T15:20:11.653924100Z" level=warning msg="failed to read oom_kill event" error="open /sys/fs/cgroup/docker/buildkit/n10ll62f2oabyoov4vegharui/memory.events: no such file or directory" span="[4/4] RUN pip install --no-cache-dir -r requirements.txt" spanID=1dbcfb3958234ca5 traceID=88d2c0c73526efcf13779fe50adbf723
Apr 14 15:20:12 minikube dockerd[13601]: time="2025-04-14T15:20:12.601970450Z" level=error msg=/moby.buildkit.v1.Control/Solve error="rpc error: code = Unknown desc = process \"/bin/sh -c pip install --no-cache-dir -r requirements.txt\" did not complete successfully: exit code: 1" spanID=479f44c3441cdd58 traceID=88d2c0c73526efcf13779fe50adbf723
Apr 14 15:20:42 minikube dockerd[13601]: time="2025-04-14T15:20:42.706704661Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:20:42 minikube dockerd[13601]: time="2025-04-14T15:20:42.706832611Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:24:16 minikube dockerd[13601]: time="2025-04-14T15:24:16.666353740Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:24:16 minikube dockerd[13601]: time="2025-04-14T15:24:16.666569921Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:25:03 minikube dockerd[13601]: time="2025-04-14T15:25:03.489451001Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:25:03 minikube dockerd[13601]: time="2025-04-14T15:25:03.491580264Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:25:48 minikube dockerd[13601]: time="2025-04-14T15:25:48.647258300Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:25:48 minikube dockerd[13601]: time="2025-04-14T15:25:48.647405650Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:26:55 minikube cri-dockerd[13922]: time="2025-04-14T15:26:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a019ea4d769c69da44f163b451b20e30e116c8c914f877455fc3c7ba9030fb39/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 14 15:26:55 minikube dockerd[13601]: time="2025-04-14T15:26:55.910461092Z" level=info msg="ignoring event" container=9562ebe472c18fe59fd7218fc879b68009c60f22018e62e9e703fed2b3342dab module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 15:26:57 minikube dockerd[13601]: time="2025-04-14T15:26:57.061453623Z" level=info msg="ignoring event" container=1bd00a3cf1ec87db7898ae332bbdc4ce7da6671b32822053e3ffab8cef2df919 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 15:27:13 minikube dockerd[13601]: time="2025-04-14T15:27:13.531070129Z" level=info msg="ignoring event" container=cce98d1549dd7db8fbb9cf4837c82eb023362e1a30d81043240b1ea929ea9ca0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 15:27:40 minikube dockerd[13601]: time="2025-04-14T15:27:40.606560221Z" level=info msg="ignoring event" container=4e5a2c5a44185882fd4681d0e7d2f39e411f3b10d7ac783e6f1422de56f18938 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 15:28:21 minikube dockerd[13601]: time="2025-04-14T15:28:21.701029424Z" level=info msg="ignoring event" container=58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 14 15:29:29 minikube dockerd[13601]: time="2025-04-14T15:29:29.516573914Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 14 15:29:29 minikube dockerd[13601]: time="2025-04-14T15:29:29.516637827Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 14 15:29:42 minikube dockerd[13601]: time="2025-04-14T15:29:42.552105829Z" level=info msg="ignoring event" container=ba4a75831fd89e8df3a1184a0e07a3f77e73f119bd956c4bb1f20147679944f4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ba4a75831fd89       d4a6fbcccf215       13 seconds ago      Exited              python-jupyter-app        5                   a019ea4d769c6       python-jupyter-deployment-7774f97799-htmlv
439da701fbeb3       6e38f40d628db       23 minutes ago      Running             storage-provisioner       5                   3d6035840eba1       storage-provisioner
51b35bd6202dc       a389e107f4ff1       23 minutes ago      Running             kube-scheduler            2                   1818c0c8ffccd       kube-scheduler-minikube
8d77dfdac7b84       c69fa2e9cbf5f       23 minutes ago      Running             coredns                   2                   1464eb2f77118       coredns-668d6bf9bc-8fttd
6e9a7d2e195e3       040f9f8aac8cd       23 minutes ago      Running             kube-proxy                2                   1ba1bbe836e1d       kube-proxy-fxbgl
5d498173d4f18       6e38f40d628db       23 minutes ago      Exited              storage-provisioner       4                   3d6035840eba1       storage-provisioner
30eddf8179e06       c2e17b8d0f4a3       23 minutes ago      Running             kube-apiserver            2                   53c38397730b2       kube-apiserver-minikube
0e36a2885af6b       a9e7e6b294baf       23 minutes ago      Running             etcd                      2                   6116ed8f7f03a       etcd-minikube
3ce843986ad60       8cab3d2a8bd0f       23 minutes ago      Running             kube-controller-manager   2                   45c3327e1fc52       kube-controller-manager-minikube
3042ea5e07606       c69fa2e9cbf5f       About an hour ago   Exited              coredns                   1                   9b490d8036590       coredns-668d6bf9bc-8fttd
8a3352176a851       040f9f8aac8cd       About an hour ago   Exited              kube-proxy                1                   3115f76ee7435       kube-proxy-fxbgl
27ff0c06f994d       8cab3d2a8bd0f       About an hour ago   Exited              kube-controller-manager   1                   3dee1cf9a20f6       kube-controller-manager-minikube
0f762f49670dc       a389e107f4ff1       About an hour ago   Exited              kube-scheduler            1                   19c3d90713c6b       kube-scheduler-minikube
64a8650b49f7c       a9e7e6b294baf       About an hour ago   Exited              etcd                      1                   08f27bcf36dd6       etcd-minikube
b1f202241698b       c2e17b8d0f4a3       About an hour ago   Exited              kube-apiserver            1                   630f21c02231c       kube-apiserver-minikube


==> coredns [3042ea5e0760] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:35161 - 20898 "HINFO IN 7390983371834986199.2573466185132092154. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.039513353s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[636703206]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 14:22:05.435) (total time: 21099ms):
Trace[636703206]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21096ms (14:22:26.532)
Trace[636703206]: [21.099938764s] [21.099938764s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[719898339]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 14:22:05.435) (total time: 21100ms):
Trace[719898339]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21097ms (14:22:26.532)
Trace[719898339]: [21.100816894s] [21.100816894s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1857970936]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (14-Apr-2025 14:22:05.435) (total time: 21100ms):
Trace[1857970936]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21097ms (14:22:26.532)
Trace[1857970936]: [21.100530219s] [21.100530219s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.744656623s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [8d77dfdac7b8] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:58060 - 46885 "HINFO IN 3518808689862437286.7155429165380354819. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.374718353s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_04_07T20_02_42_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 07 Apr 2025 14:32:31 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 14 Apr 2025 15:29:55 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 14 Apr 2025 15:24:05 +0000   Mon, 07 Apr 2025 14:32:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 14 Apr 2025 15:24:05 +0000   Mon, 07 Apr 2025 14:32:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 14 Apr 2025 15:24:05 +0000   Mon, 07 Apr 2025 14:32:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 14 Apr 2025 15:24:05 +0000   Mon, 07 Apr 2025 14:32:32 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4012920Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             4012920Ki
  pods:               110
System Info:
  Machine ID:                 a497bd779c82468baf9e4eaabfb524c1
  System UUID:                a497bd779c82468baf9e4eaabfb524c1
  Boot ID:                    1de62629-68ba-450e-8d29-07bb5fd3a91f
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     myapp-deployment-7448ddcf95-tvq2s             0 (0%)        0 (0%)      0 (0%)           0 (0%)         57m
  default                     myapp-deployment-7df474b884-4zz57             0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m
  default                     python-jupyter-deployment-64958bf866-5dq88    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m
  default                     python-jupyter-deployment-7774f97799-htmlv    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m1s
  kube-system                 coredns-668d6bf9bc-8fttd                      100m (2%)     0 (0%)      70Mi (1%)        170Mi (4%)     7d
  kube-system                 etcd-minikube                                 100m (2%)     0 (0%)      100Mi (2%)       0 (0%)         7d
  kube-system                 kube-apiserver-minikube                       250m (6%)     0 (0%)      0 (0%)           0 (0%)         7d
  kube-system                 kube-controller-manager-minikube              200m (5%)     0 (0%)      0 (0%)           0 (0%)         7d
  kube-system                 kube-proxy-fxbgl                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d
  kube-system                 kube-scheduler-minikube                       100m (2%)     0 (0%)      0 (0%)           0 (0%)         7d
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         7d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           7d                 kube-proxy       
  Normal   Starting                           23m                kube-proxy       
  Normal   Starting                           67m                kube-proxy       
  Normal   NodeHasSufficientMemory            7d (x8 over 7d)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              7d (x8 over 7d)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               7d (x7 over 7d)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            7d                 kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  7d                 kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           7d                 kubelet          Starting kubelet.
  Warning  CgroupV1                           7d                 kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            7d                 kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure              7d                 kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               7d                 kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory            7d                 kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   RegisteredNode                     7d                 node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  68m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeHasSufficientMemory            68m (x8 over 68m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Warning  CgroupV1                           68m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   Starting                           68m                kubelet          Starting kubelet.
  Normal   NodeHasNoDiskPressure              68m (x8 over 68m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               68m (x7 over 68m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            68m                kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                           67m                kubelet          Node minikube has been rebooted, boot id: 1de62629-68ba-450e-8d29-07bb5fd3a91f
  Normal   RegisteredNode                     67m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   RegisteredNode                     23m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Apr14 12:00] PCI: Fatal: No config space access function found
[  +0.071351] PCI: System does not support PCI
[  +0.112677] kvm: no hardware support
[  +0.000004] kvm: no hardware support
[  +4.547331] FS-Cache: Duplicate cookie detected
[  +0.001688] FS-Cache: O-cookie c=0000000b [p=00000002 fl=222 nc=0 na=1]
[  +0.001611] FS-Cache: O-cookie d=0000000014b15ceb{9P.session} n=00000000e44c5b26
[  +0.001466] FS-Cache: O-key=[10] '34323934393337373731'
[  +0.001437] FS-Cache: N-cookie c=0000000c [p=00000002 fl=2 nc=0 na=1]
[  +0.000917] FS-Cache: N-cookie d=0000000014b15ceb{9P.session} n=0000000089146911
[  +0.001006] FS-Cache: N-key=[10] '34323934393337373731'
[  +0.085581] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.108590] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +2.551847] netlink: 'init': attribute type 4 has an invalid length.
[  +0.228095] WSL (122) ERROR: CheckConnection: getaddrinfo() failed: -5
[Apr14 13:57] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Apr14 14:02] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.024939] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +1.492622] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[Apr14 14:21] tmpfs: Unknown parameter 'noswap'
[Apr14 14:34] hrtimer: interrupt took 550311 ns


==> etcd [0e36a2885af6] <==
{"level":"info","ts":"2025-04-14T15:06:49.183969Z","caller":"traceutil/trace.go:171","msg":"trace[243106098] range","detail":"{range_begin:/registry/replicasets/default/myapp-deployment-7df474b884; range_end:; response_count:1; response_revision:4872; }","duration":"157.863977ms","start":"2025-04-14T15:06:49.026098Z","end":"2025-04-14T15:06:49.183962Z","steps":["trace[243106098] 'agreement among raft nodes before linearized reading'  (duration: 157.727557ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:06:49.184096Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"158.057506ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/csinodes/minikube\" limit:1 ","response":"range_response_count:1 size:694"}
{"level":"info","ts":"2025-04-14T15:06:49.184186Z","caller":"traceutil/trace.go:171","msg":"trace[1890020857] range","detail":"{range_begin:/registry/csinodes/minikube; range_end:; response_count:1; response_revision:4872; }","duration":"158.15742ms","start":"2025-04-14T15:06:49.026021Z","end":"2025-04-14T15:06:49.184179Z","steps":["trace[1890020857] 'agreement among raft nodes before linearized reading'  (duration: 158.032202ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:06:49.184307Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"158.421459ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner\" limit:1 ","response":"range_response_count:1 size:238"}
{"level":"info","ts":"2025-04-14T15:06:49.184399Z","caller":"traceutil/trace.go:171","msg":"trace[1473175128] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner; range_end:; response_count:1; response_revision:4872; }","duration":"158.520475ms","start":"2025-04-14T15:06:49.025870Z","end":"2025-04-14T15:06:49.184391Z","steps":["trace[1473175128] 'agreement among raft nodes before linearized reading'  (duration: 158.395456ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:06:49.184516Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"158.6979ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/replicasets/default/myapp-deployment-7448ddcf95\" limit:1 ","response":"range_response_count:1 size:1868"}
{"level":"info","ts":"2025-04-14T15:06:49.184601Z","caller":"traceutil/trace.go:171","msg":"trace[1704600187] range","detail":"{range_begin:/registry/replicasets/default/myapp-deployment-7448ddcf95; range_end:; response_count:1; response_revision:4872; }","duration":"158.791515ms","start":"2025-04-14T15:06:49.025803Z","end":"2025-04-14T15:06:49.184595Z","steps":["trace[1704600187] 'agreement among raft nodes before linearized reading'  (duration: 158.673497ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:06:49.184714Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"158.918534ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/kube-system/kube-proxy\" limit:1 ","response":"range_response_count:1 size:2895"}
{"level":"info","ts":"2025-04-14T15:06:49.184728Z","caller":"traceutil/trace.go:171","msg":"trace[1668796672] range","detail":"{range_begin:/registry/daemonsets/kube-system/kube-proxy; range_end:; response_count:1; response_revision:4872; }","duration":"158.951838ms","start":"2025-04-14T15:06:49.025772Z","end":"2025-04-14T15:06:49.184724Z","steps":["trace[1668796672] 'agreement among raft nodes before linearized reading'  (duration: 158.902731ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:07:56.381060Z","caller":"traceutil/trace.go:171","msg":"trace[700872941] transaction","detail":"{read_only:false; response_revision:4941; number_of_response:1; }","duration":"118.090216ms","start":"2025-04-14T15:07:56.262953Z","end":"2025-04-14T15:07:56.381043Z","steps":["trace[700872941] 'process raft request'  (duration: 117.932391ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:08:00.521098Z","caller":"traceutil/trace.go:171","msg":"trace[1341479220] transaction","detail":"{read_only:false; response_revision:4945; number_of_response:1; }","duration":"112.372797ms","start":"2025-04-14T15:08:00.408694Z","end":"2025-04-14T15:08:00.521066Z","steps":["trace[1341479220] 'process raft request'  (duration: 111.936539ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:14:51.635038Z","caller":"traceutil/trace.go:171","msg":"trace[47333400] transaction","detail":"{read_only:false; response_revision:5325; number_of_response:1; }","duration":"143.051985ms","start":"2025-04-14T15:14:51.491853Z","end":"2025-04-14T15:14:51.634905Z","steps":["trace[47333400] 'process raft request'  (duration: 142.846299ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:16:00.628298Z","caller":"traceutil/trace.go:171","msg":"trace[1807399135] transaction","detail":"{read_only:false; response_revision:5396; number_of_response:1; }","duration":"115.494185ms","start":"2025-04-14T15:16:00.512782Z","end":"2025-04-14T15:16:00.628276Z","steps":["trace[1807399135] 'process raft request'  (duration: 115.222979ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:16:32.600334Z","caller":"traceutil/trace.go:171","msg":"trace[377783356] linearizableReadLoop","detail":"{readStateIndex:6617; appliedIndex:6616; }","duration":"125.855058ms","start":"2025-04-14T15:16:32.474392Z","end":"2025-04-14T15:16:32.600247Z","steps":["trace[377783356] 'read index received'  (duration: 32.419443ms)","trace[377783356] 'applied index is now lower than readState.Index'  (duration: 93.434715ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-14T15:16:32.600807Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.13617ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:16:32.600904Z","caller":"traceutil/trace.go:171","msg":"trace[14898333] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5426; }","duration":"126.522323ms","start":"2025-04-14T15:16:32.474362Z","end":"2025-04-14T15:16:32.600885Z","steps":["trace[14898333] 'agreement among raft nodes before linearized reading'  (duration: 126.047935ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:16:39.668185Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5122}
{"level":"info","ts":"2025-04-14T15:16:39.691857Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5122,"took":"23.080927ms","hash":2105343440,"current-db-size-bytes":3014656,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1753088,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-14T15:16:39.691935Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2105343440,"revision":5122,"compact-revision":4307}
{"level":"info","ts":"2025-04-14T15:16:58.265980Z","caller":"traceutil/trace.go:171","msg":"trace[1575655409] transaction","detail":"{read_only:false; response_revision:5455; number_of_response:1; }","duration":"143.092965ms","start":"2025-04-14T15:16:58.122431Z","end":"2025-04-14T15:16:58.265524Z","steps":["trace[1575655409] 'process raft request'  (duration: 142.99333ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:19:51.135391Z","caller":"traceutil/trace.go:171","msg":"trace[2002814865] transaction","detail":"{read_only:false; response_revision:5607; number_of_response:1; }","duration":"109.040243ms","start":"2025-04-14T15:19:51.026185Z","end":"2025-04-14T15:19:51.135226Z","steps":["trace[2002814865] 'process raft request'  (duration: 108.885683ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:19:56.095625Z","caller":"traceutil/trace.go:171","msg":"trace[1180746362] range","detail":"{range_begin:/registry/persistentvolumeclaims/; range_end:/registry/persistentvolumeclaims0; response_count:0; response_revision:5610; }","duration":"127.763962ms","start":"2025-04-14T15:19:55.967018Z","end":"2025-04-14T15:19:56.094782Z","steps":["trace[1180746362] 'agreement among raft nodes before linearized reading'  (duration: 127.730849ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:20:00.185041Z","caller":"traceutil/trace.go:171","msg":"trace[1813441522] transaction","detail":"{read_only:false; response_revision:5613; number_of_response:1; }","duration":"227.872077ms","start":"2025-04-14T15:19:59.967484Z","end":"2025-04-14T15:20:00.185020Z","steps":["trace[1813441522] 'process raft request'  (duration: 227.583368ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:20:00.241121Z","caller":"traceutil/trace.go:171","msg":"trace[492031055] transaction","detail":"{read_only:false; response_revision:5614; number_of_response:1; }","duration":"283.863078ms","start":"2025-04-14T15:19:59.967574Z","end":"2025-04-14T15:20:00.241102Z","steps":["trace[492031055] 'process raft request'  (duration: 283.704218ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:20:00.658686Z","caller":"traceutil/trace.go:171","msg":"trace[38423542] range","detail":"{range_begin:/registry/volumeattachments/; range_end:/registry/volumeattachments0; response_count:0; response_revision:5614; }","duration":"139.439049ms","start":"2025-04-14T15:20:00.427533Z","end":"2025-04-14T15:20:00.566972Z","steps":["trace[38423542] 'agreement among raft nodes before linearized reading'  (duration: 139.345114ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:20:01.375608Z","caller":"traceutil/trace.go:171","msg":"trace[773771954] transaction","detail":"{read_only:false; response_revision:5615; number_of_response:1; }","duration":"120.824535ms","start":"2025-04-14T15:20:01.254765Z","end":"2025-04-14T15:20:01.375590Z","steps":["trace[773771954] 'process raft request'  (duration: 117.91814ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:20:07.580361Z","caller":"traceutil/trace.go:171","msg":"trace[1927238200] transaction","detail":"{read_only:false; response_revision:5621; number_of_response:1; }","duration":"151.562648ms","start":"2025-04-14T15:20:07.428756Z","end":"2025-04-14T15:20:07.580319Z","steps":["trace[1927238200] 'process raft request'  (duration: 151.348364ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:20:11.812679Z","caller":"traceutil/trace.go:171","msg":"trace[438758891] transaction","detail":"{read_only:false; response_revision:5624; number_of_response:1; }","duration":"200.117063ms","start":"2025-04-14T15:20:11.612539Z","end":"2025-04-14T15:20:11.812656Z","steps":["trace[438758891] 'process raft request'  (duration: 199.669189ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:20:11.813421Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"177.451133ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:20:11.813479Z","caller":"traceutil/trace.go:171","msg":"trace[1053689913] linearizableReadLoop","detail":"{readStateIndex:6861; appliedIndex:6860; }","duration":"176.865505ms","start":"2025-04-14T15:20:11.635626Z","end":"2025-04-14T15:20:11.812492Z","steps":["trace[1053689913] 'read index received'  (duration: 176.465349ms)","trace[1053689913] 'applied index is now lower than readState.Index'  (duration: 351.437¬µs)"],"step_count":2}
{"level":"info","ts":"2025-04-14T15:20:11.813547Z","caller":"traceutil/trace.go:171","msg":"trace[1743702247] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:5624; }","duration":"177.913614ms","start":"2025-04-14T15:20:11.635620Z","end":"2025-04-14T15:20:11.813533Z","steps":["trace[1743702247] 'agreement among raft nodes before linearized reading'  (duration: 177.318282ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:21:39.563733Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5434}
{"level":"info","ts":"2025-04-14T15:21:39.645527Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5434,"took":"80.643196ms","hash":1092337053,"current-db-size-bytes":3014656,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1941504,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-04-14T15:21:39.645687Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1092337053,"revision":5434,"compact-revision":5122}
{"level":"info","ts":"2025-04-14T15:22:00.646617Z","caller":"traceutil/trace.go:171","msg":"trace[1443956415] transaction","detail":"{read_only:false; response_revision:5716; number_of_response:1; }","duration":"124.630807ms","start":"2025-04-14T15:22:00.521840Z","end":"2025-04-14T15:22:00.646471Z","steps":["trace[1443956415] 'process raft request'  (duration: 124.469556ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:22:03.093612Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"225.119242ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036581178473336 > lease_revoke:<id:70cc9634d685fb2a>","response":"size:29"}
{"level":"info","ts":"2025-04-14T15:22:03.096850Z","caller":"traceutil/trace.go:171","msg":"trace[856298326] linearizableReadLoop","detail":"{readStateIndex:6978; appliedIndex:6977; }","duration":"499.289973ms","start":"2025-04-14T15:22:02.597348Z","end":"2025-04-14T15:22:03.096638Z","steps":["trace[856298326] 'read index received'  (duration: 177.641954ms)","trace[856298326] 'applied index is now lower than readState.Index'  (duration: 321.616809ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-14T15:22:03.110774Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"176.470582ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/\" range_end:\"/registry/limitranges0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:22:03.111007Z","caller":"traceutil/trace.go:171","msg":"trace[1590816080] range","detail":"{range_begin:/registry/limitranges/; range_end:/registry/limitranges0; response_count:0; response_revision:5716; }","duration":"177.333056ms","start":"2025-04-14T15:22:02.933612Z","end":"2025-04-14T15:22:03.110946Z","steps":["trace[1590816080] 'agreement among raft nodes before linearized reading'  (duration: 175.938913ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:22:03.111214Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"358.117109ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-04-14T15:22:03.111380Z","caller":"traceutil/trace.go:171","msg":"trace[979733828] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:5716; }","duration":"358.551446ms","start":"2025-04-14T15:22:02.752779Z","end":"2025-04-14T15:22:03.111330Z","steps":["trace[979733828] 'agreement among raft nodes before linearized reading'  (duration: 356.644641ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:22:03.113371Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"516.023991ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:22:03.113446Z","caller":"traceutil/trace.go:171","msg":"trace[232741939] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:5716; }","duration":"516.08451ms","start":"2025-04-14T15:22:02.597316Z","end":"2025-04-14T15:22:03.113400Z","steps":["trace[232741939] 'agreement among raft nodes before linearized reading'  (duration: 500.22547ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:22:03.111455Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:22:02.752626Z","time spent":"358.81193ms","remote":"127.0.0.1:37290","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"info","ts":"2025-04-14T15:22:03.465973Z","caller":"traceutil/trace.go:171","msg":"trace[848046228] linearizableReadLoop","detail":"{readStateIndex:6979; appliedIndex:6978; }","duration":"110.503617ms","start":"2025-04-14T15:22:03.355244Z","end":"2025-04-14T15:22:03.465747Z","steps":["trace[848046228] 'read index received'  (duration: 109.468989ms)","trace[848046228] 'applied index is now lower than readState.Index'  (duration: 1.033928ms)"],"step_count":2}
{"level":"info","ts":"2025-04-14T15:22:03.466094Z","caller":"traceutil/trace.go:171","msg":"trace[1758802439] transaction","detail":"{read_only:false; response_revision:5717; number_of_response:1; }","duration":"159.867105ms","start":"2025-04-14T15:22:03.306209Z","end":"2025-04-14T15:22:03.466076Z","steps":["trace[1758802439] 'process raft request'  (duration: 158.538083ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:22:03.466369Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.110411ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:22:03.466404Z","caller":"traceutil/trace.go:171","msg":"trace[1704735506] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5717; }","duration":"111.195938ms","start":"2025-04-14T15:22:03.355191Z","end":"2025-04-14T15:22:03.466387Z","steps":["trace[1704735506] 'agreement among raft nodes before linearized reading'  (duration: 110.860631ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:22:05.184768Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"174.810454ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036581178473346 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:5709 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:16"}
{"level":"info","ts":"2025-04-14T15:22:05.185943Z","caller":"traceutil/trace.go:171","msg":"trace[405544360] transaction","detail":"{read_only:false; response_revision:5719; number_of_response:1; }","duration":"500.310998ms","start":"2025-04-14T15:22:04.685527Z","end":"2025-04-14T15:22:05.185838Z","steps":["trace[405544360] 'process raft request'  (duration: 293.649521ms)","trace[405544360] 'compare'  (duration: 171.642548ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-14T15:22:05.186388Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:22:04.685356Z","time spent":"500.9502ms","remote":"127.0.0.1:37360","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:5709 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2025-04-14T15:22:36.178197Z","caller":"traceutil/trace.go:171","msg":"trace[1792933786] transaction","detail":"{read_only:false; response_revision:5746; number_of_response:1; }","duration":"152.498161ms","start":"2025-04-14T15:22:36.025680Z","end":"2025-04-14T15:22:36.178178Z","steps":["trace[1792933786] 'process raft request'  (duration: 152.314813ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:22:46.529961Z","caller":"traceutil/trace.go:171","msg":"trace[53823613] transaction","detail":"{read_only:false; response_revision:5754; number_of_response:1; }","duration":"128.151617ms","start":"2025-04-14T15:22:46.400108Z","end":"2025-04-14T15:22:46.528260Z","steps":["trace[53823613] 'process raft request'  (duration: 128.039463ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:22:52.517551Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"238.575834ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036581178473579 > lease_revoke:<id:70cc9634d685fc21>","response":"size:29"}
{"level":"info","ts":"2025-04-14T15:22:52.519158Z","caller":"traceutil/trace.go:171","msg":"trace[510563889] linearizableReadLoop","detail":"{readStateIndex:7029; appliedIndex:7028; }","duration":"210.328886ms","start":"2025-04-14T15:22:52.308384Z","end":"2025-04-14T15:22:52.518713Z","steps":["trace[510563889] 'read index received'  (duration: 30.714¬µs)","trace[510563889] 'applied index is now lower than readState.Index'  (duration: 210.296872ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-14T15:22:52.519809Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"211.229583ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingwebhookconfigurations/\" range_end:\"/registry/validatingwebhookconfigurations0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:22:52.521471Z","caller":"traceutil/trace.go:171","msg":"trace[1166314722] range","detail":"{range_begin:/registry/validatingwebhookconfigurations/; range_end:/registry/validatingwebhookconfigurations0; response_count:0; response_revision:5757; }","duration":"213.117815ms","start":"2025-04-14T15:22:52.308316Z","end":"2025-04-14T15:22:52.521433Z","steps":["trace[1166314722] 'agreement among raft nodes before linearized reading'  (duration: 211.138543ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:26:39.489802Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5697}
{"level":"info","ts":"2025-04-14T15:26:39.656348Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":5697,"took":"163.740187ms","hash":2023192417,"current-db-size-bytes":3014656,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1789952,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-04-14T15:26:39.656588Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2023192417,"revision":5697,"compact-revision":5434}


==> etcd [64a8650b49f7] <==
{"level":"info","ts":"2025-04-14T15:01:47.702804Z","caller":"traceutil/trace.go:171","msg":"trace[1663517549] transaction","detail":"{read_only:false; response_revision:4537; number_of_response:1; }","duration":"154.686889ms","start":"2025-04-14T15:01:47.548068Z","end":"2025-04-14T15:01:47.702755Z","steps":["trace[1663517549] 'process raft request'  (duration: 131.620007ms)","trace[1663517549] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 22.919844ms)"],"step_count":2}
{"level":"info","ts":"2025-04-14T15:01:57.526871Z","caller":"traceutil/trace.go:171","msg":"trace[88136713] transaction","detail":"{read_only:false; response_revision:4546; number_of_response:1; }","duration":"130.669583ms","start":"2025-04-14T15:01:57.396182Z","end":"2025-04-14T15:01:57.526852Z","steps":["trace[88136713] 'process raft request'  (duration: 121.244217ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:01:57.615227Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4307}
{"level":"info","ts":"2025-04-14T15:01:58.344545Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":4307,"took":"727.387485ms","hash":756588574,"current-db-size-bytes":2867200,"current-db-size":"2.9 MB","current-db-size-in-use-bytes":1564672,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-04-14T15:01:58.344701Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":756588574,"revision":4307,"compact-revision":4061}
{"level":"warn","ts":"2025-04-14T15:01:58.345914Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"517.388539ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036580492310951 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:4545 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2025-04-14T15:01:58.346104Z","caller":"traceutil/trace.go:171","msg":"trace[87152462] linearizableReadLoop","detail":"{readStateIndex:5560; appliedIndex:5559; }","duration":"482.00921ms","start":"2025-04-14T15:01:57.864054Z","end":"2025-04-14T15:01:58.346063Z","steps":["trace[87152462] 'read index received'  (duration: 26.105¬µs)","trace[87152462] 'applied index is now lower than readState.Index'  (duration: 481.981805ms)"],"step_count":2}
{"level":"info","ts":"2025-04-14T15:01:58.346215Z","caller":"traceutil/trace.go:171","msg":"trace[1672339110] transaction","detail":"{read_only:false; response_revision:4547; number_of_response:1; }","duration":"524.018682ms","start":"2025-04-14T15:01:57.822177Z","end":"2025-04-14T15:01:58.346196Z","steps":["trace[1672339110] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 517.009268ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:01:58.346279Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:01:57.822158Z","time spent":"524.077892ms","remote":"127.0.0.1:52278","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:4545 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-04-14T15:01:58.346509Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"482.43319ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:01:58.346540Z","caller":"traceutil/trace.go:171","msg":"trace[703894737] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4547; }","duration":"482.498802ms","start":"2025-04-14T15:01:57.864025Z","end":"2025-04-14T15:01:58.346524Z","steps":["trace[703894737] 'agreement among raft nodes before linearized reading'  (duration: 482.415887ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:01:58.346561Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:01:57.864009Z","time spent":"482.546611ms","remote":"127.0.0.1:52098","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-04-14T15:01:58.346682Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"439.568359ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:01:58.346699Z","caller":"traceutil/trace.go:171","msg":"trace[22826741] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4547; }","duration":"439.601965ms","start":"2025-04-14T15:01:57.907092Z","end":"2025-04-14T15:01:58.346694Z","steps":["trace[22826741] 'agreement among raft nodes before linearized reading'  (duration: 439.547955ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:02:04.604185Z","caller":"traceutil/trace.go:171","msg":"trace[1013737410] transaction","detail":"{read_only:false; response_revision:4553; number_of_response:1; }","duration":"145.079952ms","start":"2025-04-14T15:02:04.459089Z","end":"2025-04-14T15:02:04.604169Z","steps":["trace[1013737410] 'process raft request'  (duration: 144.904818ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:02:08.817803Z","caller":"traceutil/trace.go:171","msg":"trace[1530808574] transaction","detail":"{read_only:false; response_revision:4556; number_of_response:1; }","duration":"138.784943ms","start":"2025-04-14T15:02:08.679001Z","end":"2025-04-14T15:02:08.817786Z","steps":["trace[1530808574] 'process raft request'  (duration: 138.393068ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:02:28.852031Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"169.22479ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/myapp-deployment-7448ddcf95-tvq2s.183635e0453091e3\" limit:1 ","response":"range_response_count:1 size:756"}
{"level":"info","ts":"2025-04-14T15:02:28.852226Z","caller":"traceutil/trace.go:171","msg":"trace[1676494274] range","detail":"{range_begin:/registry/events/default/myapp-deployment-7448ddcf95-tvq2s.183635e0453091e3; range_end:; response_count:1; response_revision:4571; }","duration":"169.49684ms","start":"2025-04-14T15:02:28.682712Z","end":"2025-04-14T15:02:28.852209Z","steps":["trace[1676494274] 'range keys from bolt db'  (duration: 169.152004ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:02:29.094741Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"123.086408ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/default/myapp-deployment-7448ddcf95-tvq2s.183635e045323b5a\" limit:1 ","response":"range_response_count:1 size:749"}
{"level":"info","ts":"2025-04-14T15:02:29.095486Z","caller":"traceutil/trace.go:171","msg":"trace[929349809] range","detail":"{range_begin:/registry/events/default/myapp-deployment-7448ddcf95-tvq2s.183635e045323b5a; range_end:; response_count:1; response_revision:4572; }","duration":"124.367468ms","start":"2025-04-14T15:02:28.971095Z","end":"2025-04-14T15:02:29.095462Z","steps":["trace[929349809] 'range keys from in-memory index tree'  (duration: 89.094957ms)","trace[929349809] 'range keys from bolt db'  (duration: 33.934262ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-14T15:04:13.785402Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.144169ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:04:13.789435Z","caller":"traceutil/trace.go:171","msg":"trace[630428306] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4656; }","duration":"115.23842ms","start":"2025-04-14T15:04:13.670720Z","end":"2025-04-14T15:04:13.785958Z","steps":["trace[630428306] 'range keys from in-memory index tree'  (duration: 112.103863ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:04:16.439848Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.371412ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128036580492311621 > lease_revoke:<id:70cc9634ad9ffbf8>","response":"size:29"}
{"level":"info","ts":"2025-04-14T15:04:21.139165Z","caller":"traceutil/trace.go:171","msg":"trace[21080040] linearizableReadLoop","detail":"{readStateIndex:5703; appliedIndex:5702; }","duration":"251.556472ms","start":"2025-04-14T15:04:20.887563Z","end":"2025-04-14T15:04:21.139119Z","steps":["trace[21080040] 'read index received'  (duration: 245.960274ms)","trace[21080040] 'applied index is now lower than readState.Index'  (duration: 5.595297ms)"],"step_count":2}
{"level":"warn","ts":"2025-04-14T15:04:21.139345Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:04:20.838122Z","time spent":"301.207475ms","remote":"127.0.0.1:52130","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2025-04-14T15:04:21.167759Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"252.254646ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:04:21.167913Z","caller":"traceutil/trace.go:171","msg":"trace[81360178] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4661; }","duration":"280.294042ms","start":"2025-04-14T15:04:20.887556Z","end":"2025-04-14T15:04:21.167850Z","steps":["trace[81360178] 'agreement among raft nodes before linearized reading'  (duration: 252.202741ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:04:21.534655Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"328.538795ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2025-04-14T15:04:21.575082Z","caller":"traceutil/trace.go:171","msg":"trace[1805039440] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:4662; }","duration":"369.084426ms","start":"2025-04-14T15:04:21.205975Z","end":"2025-04-14T15:04:21.575060Z","steps":["trace[1805039440] 'range keys from in-memory index tree'  (duration: 328.358376ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:04:21.575168Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:04:21.205933Z","time spent":"369.203739ms","remote":"127.0.0.1:52278","response type":"/etcdserverpb.KV/Range","request count":0,"request size":51,"response count":1,"response size":444,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" limit:1 "}
{"level":"warn","ts":"2025-04-14T15:04:49.258146Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"332.411586ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-04-14T15:04:49.258469Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"378.017682ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-04-14T15:04:49.265266Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"355.76721ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-04-14T15:04:49.569411Z","caller":"traceutil/trace.go:171","msg":"trace[1412212305] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4685; }","duration":"419.95657ms","start":"2025-04-14T15:04:48.838551Z","end":"2025-04-14T15:04:49.258507Z","steps":["trace[1412212305] 'range keys from in-memory index tree'  (duration: 377.970675ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:04:49.569550Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:04:48.838534Z","time spent":"730.987805ms","remote":"127.0.0.1:52098","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-04-14T15:04:49.570779Z","caller":"traceutil/trace.go:171","msg":"trace[897449553] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4685; }","duration":"374.301268ms","start":"2025-04-14T15:04:48.883895Z","end":"2025-04-14T15:04:49.258196Z","steps":["trace[897449553] 'range keys from in-memory index tree'  (duration: 332.405686ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:04:49.571453Z","caller":"traceutil/trace.go:171","msg":"trace[477440264] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:4685; }","duration":"404.845485ms","start":"2025-04-14T15:04:48.860464Z","end":"2025-04-14T15:04:49.265309Z","steps":["trace[477440264] 'range keys from in-memory index tree'  (duration: 355.55318ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:04:49.571512Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:04:48.860448Z","time spent":"711.044953ms","remote":"127.0.0.1:52278","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"info","ts":"2025-04-14T15:04:49.955374Z","caller":"traceutil/trace.go:171","msg":"trace[384788865] transaction","detail":"{read_only:false; response_revision:4686; number_of_response:1; }","duration":"300.732213ms","start":"2025-04-14T15:04:49.654622Z","end":"2025-04-14T15:04:49.955355Z","steps":["trace[384788865] 'process raft request'  (duration: 300.049318ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:04:50.002918Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-04-14T15:04:49.654602Z","time spent":"300.893135ms","remote":"127.0.0.1:52278","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:4682 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-04-14T15:05:00.237151Z","caller":"traceutil/trace.go:171","msg":"trace[1933551178] transaction","detail":"{read_only:false; response_revision:4695; number_of_response:1; }","duration":"128.366509ms","start":"2025-04-14T15:05:00.108766Z","end":"2025-04-14T15:05:00.237133Z","steps":["trace[1933551178] 'process raft request'  (duration: 128.193287ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:05:08.881834Z","caller":"traceutil/trace.go:171","msg":"trace[1226210545] transaction","detail":"{read_only:false; response_revision:4702; number_of_response:1; }","duration":"113.198583ms","start":"2025-04-14T15:05:08.768618Z","end":"2025-04-14T15:05:08.881816Z","steps":["trace[1226210545] 'process raft request'  (duration: 112.562396ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:05:32.979921Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.303145ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:05:32.979972Z","caller":"traceutil/trace.go:171","msg":"trace[229511640] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4722; }","duration":"102.367054ms","start":"2025-04-14T15:05:32.877595Z","end":"2025-04-14T15:05:32.979962Z","steps":["trace[229511640] 'range keys from in-memory index tree'  (duration: 102.293144ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:05:49.109375Z","caller":"traceutil/trace.go:171","msg":"trace[1596562673] linearizableReadLoop","detail":"{readStateIndex:5793; appliedIndex:5792; }","duration":"260.014742ms","start":"2025-04-14T15:05:48.849341Z","end":"2025-04-14T15:05:49.109356Z","steps":["trace[1596562673] 'read index received'  (duration: 259.758408ms)","trace[1596562673] 'applied index is now lower than readState.Index'  (duration: 255.734¬µs)"],"step_count":2}
{"level":"info","ts":"2025-04-14T15:05:49.109701Z","caller":"traceutil/trace.go:171","msg":"trace[1911897931] transaction","detail":"{read_only:false; response_revision:4734; number_of_response:1; }","duration":"295.274472ms","start":"2025-04-14T15:05:48.814414Z","end":"2025-04-14T15:05:49.109689Z","steps":["trace[1911897931] 'process raft request'  (duration: 294.817812ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:05:49.110171Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"260.815447ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:05:49.110670Z","caller":"traceutil/trace.go:171","msg":"trace[1930521182] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4734; }","duration":"261.324113ms","start":"2025-04-14T15:05:48.849337Z","end":"2025-04-14T15:05:49.110661Z","steps":["trace[1930521182] 'agreement among raft nodes before linearized reading'  (duration: 260.796944ms)"],"step_count":1}
{"level":"warn","ts":"2025-04-14T15:05:49.110431Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"230.141619ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-04-14T15:05:49.110845Z","caller":"traceutil/trace.go:171","msg":"trace[1344547716] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4734; }","duration":"230.567375ms","start":"2025-04-14T15:05:48.880271Z","end":"2025-04-14T15:05:49.110839Z","steps":["trace[1344547716] 'agreement among raft nodes before linearized reading'  (duration: 230.123416ms)"],"step_count":1}
{"level":"info","ts":"2025-04-14T15:06:10.243572Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-04-14T15:06:10.313557Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-04-14T15:06:10.348476Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-14T15:06:10.419823Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-14T15:06:10.575108Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-04-14T15:06:10.575211Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-04-14T15:06:10.580490Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-04-14T15:06:10.615532Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-14T15:06:10.618540Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-04-14T15:06:10.618711Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 15:29:56 up  3:29,  0 users,  load average: 1.48, 1.06, 1.35
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [30eddf8179e0] <==
W0414 15:06:42.276299       1 genericapiserver.go:767] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0414 15:06:44.160118       1 secure_serving.go:213] Serving securely on [::]:8443
I0414 15:06:44.160497       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0414 15:06:44.173391       1 controller.go:78] Starting OpenAPI AggregationController
I0414 15:06:44.173638       1 local_available_controller.go:156] Starting LocalAvailability controller
I0414 15:06:44.173666       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0414 15:06:44.174354       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0414 15:06:44.252652       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0414 15:06:44.252826       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0414 15:06:44.255150       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0414 15:06:44.256051       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0414 15:06:44.256132       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0414 15:06:44.256412       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0414 15:06:44.256585       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0414 15:06:44.257166       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0414 15:06:44.257252       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0414 15:06:44.257283       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0414 15:06:44.257352       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0414 15:06:44.257673       1 controller.go:119] Starting legacy_token_tracking_controller
I0414 15:06:44.257757       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0414 15:06:44.257787       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0414 15:06:44.262721       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0414 15:06:44.266585       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0414 15:06:44.270973       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0414 15:06:44.271054       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0414 15:06:44.271096       1 aggregator.go:169] waiting for initial CRD sync...
I0414 15:06:44.306814       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0414 15:06:44.312598       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0414 15:06:44.370526       1 controller.go:142] Starting OpenAPI controller
I0414 15:06:44.370707       1 controller.go:90] Starting OpenAPI V3 controller
I0414 15:06:44.370835       1 naming_controller.go:294] Starting NamingConditionController
I0414 15:06:44.370925       1 establishing_controller.go:81] Starting EstablishingController
I0414 15:06:44.370990       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0414 15:06:44.371108       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0414 15:06:44.371144       1 crd_finalizer.go:269] Starting CRDFinalizer
I0414 15:06:44.713995       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0414 15:06:44.720733       1 aggregator.go:171] initial CRD sync complete...
I0414 15:06:44.720825       1 autoregister_controller.go:144] Starting autoregister controller
I0414 15:06:44.720836       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0414 15:06:44.802585       1 cache.go:39] Caches are synced for LocalAvailability controller
I0414 15:06:44.805229       1 shared_informer.go:320] Caches are synced for node_authorizer
I0414 15:06:44.806647       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0414 15:06:44.806744       1 policy_source.go:240] refreshing policies
I0414 15:06:44.811375       1 shared_informer.go:320] Caches are synced for configmaps
I0414 15:06:44.811757       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0414 15:06:44.831197       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0414 15:06:44.831484       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0414 15:06:44.832576       1 cache.go:39] Caches are synced for autoregister controller
I0414 15:06:44.835322       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0414 15:06:44.836285       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0414 15:06:44.907819       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0414 15:06:44.929010       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0414 15:06:45.236172       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0414 15:06:47.395501       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0414 15:06:47.397285       1 controller.go:615] quota admission added evaluator for: endpoints
I0414 15:06:47.664605       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0414 15:06:48.955008       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0414 15:06:49.216185       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0414 15:06:49.351606       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0414 15:13:38.574817       1 alloc.go:330] "allocated clusterIPs" service="default/python-jupyter-deployment" clusterIPs={"IPv4":"10.107.231.253"}


==> kube-apiserver [b1f202241698] <==
W0414 15:06:15.814522       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.860340       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.884575       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.884543       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.910440       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.914477       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.931274       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.940229       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.945578       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.975618       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:15.986893       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:16.095402       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:16.125145       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:16.126905       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:16.192401       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:16.193436       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:16.230525       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:16.286365       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:16.309362       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:18.565032       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:18.711438       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:18.890488       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.004485       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.042883       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.054820       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.070208       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.079055       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.180646       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.220180       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.226657       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.297365       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.301798       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.329753       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.380817       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.384571       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.421170       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.421916       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.430647       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.486617       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.517463       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.531074       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.544563       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.554876       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.641466       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.703390       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.713342       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.757991       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.796485       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.911391       1 logging.go:55] [core] [Channel #6 SubChannel #7]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.912706       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.929531       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.934169       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.938681       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.946234       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.960983       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:19.970120       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:20.072768       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:20.117951       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:20.127984       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0414 15:06:20.128777       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [27ff0c06f994] <==
I0414 14:26:49.528565       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="96.022¬µs"
I0414 14:26:55.239929       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="101.721¬µs"
I0414 14:27:08.221846       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="124.524¬µs"
I0414 14:27:29.140484       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="865.215¬µs"
I0414 14:27:40.102442       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="121.121¬µs"
I0414 14:28:00.105184       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="122.921¬µs"
I0414 14:28:15.131624       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="1.997342ms"
I0414 14:28:45.118092       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="8.317853ms"
I0414 14:28:58.092098       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="306.068¬µs"
I0414 14:28:58.976691       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 14:30:20.088748       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="1.161394ms"
I0414 14:30:31.203949       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="2.659198ms"
I0414 14:32:17.399724       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="59.8973ms"
I0414 14:32:17.417614       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="17.503981ms"
I0414 14:32:17.417750       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="93.416¬µs"
I0414 14:32:17.437246       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="31.105¬µs"
I0414 14:32:23.021055       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="213.736¬µs"
I0414 14:32:34.179815       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="70.812¬µs"
I0414 14:32:49.057357       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="44.408¬µs"
I0414 14:33:00.071253       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="50.208¬µs"
I0414 14:33:13.058354       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="90.715¬µs"
I0414 14:33:18.051975       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="70.411¬µs"
I0414 14:33:27.084068       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="143.122¬µs"
I0414 14:33:30.037581       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="45.306¬µs"
I0414 14:34:04.270292       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 14:34:09.038766       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="124.32¬µs"
I0414 14:34:24.041465       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="563.492¬µs"
I0414 14:35:46.037042       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="651.016¬µs"
I0414 14:36:01.025852       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="1.104742ms"
I0414 14:38:34.984600       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="316.952¬µs"
I0414 14:38:49.003240       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="87.222¬µs"
I0414 14:39:07.008958       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="119.521¬µs"
I0414 14:39:10.980217       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 14:39:11.891544       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="72.013¬µs"
I0414 14:39:20.990967       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="46.708¬µs"
I0414 14:43:55.002550       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="162.236¬µs"
I0414 14:44:08.035921       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="64.315¬µs"
I0414 14:44:16.500763       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 14:44:22.921692       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="127.429¬µs"
I0414 14:44:37.915590       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="220.83¬µs"
I0414 14:49:11.875890       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="689.454¬µs"
I0414 14:49:22.587611       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 14:49:25.856296       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="98.723¬µs"
I0414 14:49:44.850945       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="628.636¬µs"
I0414 14:49:58.854182       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="192.442¬µs"
I0414 14:54:18.807690       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="2.091352ms"
I0414 14:54:27.578935       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 14:54:30.791083       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="1.126222ms"
I0414 14:54:56.790009       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="143.223¬µs"
I0414 14:55:11.795656       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="82.217¬µs"
I0414 14:59:31.998373       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="13.080619ms"
I0414 14:59:35.571625       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 14:59:43.782951       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="207.241¬µs"
I0414 15:00:10.858104       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="1.536887ms"
I0414 15:00:24.769429       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="131.523¬µs"
I0414 15:04:43.129044       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 15:04:46.967228       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="18.056293ms"
I0414 15:04:59.697724       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="90.912¬µs"
I0414 15:05:25.830902       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="80.611¬µs"
I0414 15:05:39.702073       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="46.606¬µs"


==> kube-controller-manager [3ce843986ad6] <==
I0414 15:06:49.229007       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="50.707¬µs"
I0414 15:06:49.658345       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="31.157994ms"
I0414 15:06:49.658573       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="78.612¬µs"
I0414 15:09:48.598317       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 15:09:56.647749       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="1.198143ms"
I0414 15:10:09.630050       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="152.518¬µs"
I0414 15:10:37.637349       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="408.254¬µs"
I0414 15:10:51.626834       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="45.806¬µs"
I0414 15:12:54.890105       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="49.610002ms"
I0414 15:12:54.946429       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="56.230116ms"
I0414 15:12:54.946529       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="51.107¬µs"
I0414 15:12:54.952081       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="76.609¬µs"
I0414 15:12:54.983785       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="29.604¬µs"
I0414 15:12:59.973296       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="75.909¬µs"
I0414 15:13:11.599560       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="83.314¬µs"
I0414 15:13:36.590769       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="130.677¬µs"
I0414 15:13:47.595853       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="190.69¬µs"
I0414 15:14:05.629761       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="159.971¬µs"
I0414 15:14:18.573894       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="322.842¬µs"
I0414 15:14:48.561350       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="231.097¬µs"
I0414 15:14:54.268935       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 15:14:59.577333       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="80.229¬µs"
I0414 15:15:03.553602       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="90.635¬µs"
I0414 15:15:14.541846       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="289.415¬µs"
I0414 15:15:47.529379       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="99.24¬µs"
I0414 15:16:00.630518       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="123.149¬µs"
I0414 15:16:26.516809       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="177.972¬µs"
I0414 15:16:40.552428       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="1.153961ms"
I0414 15:19:20.456948       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="297.313¬µs"
I0414 15:19:35.446341       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="490.185¬µs"
I0414 15:20:00.246558       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 15:20:07.597967       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="425.865¬µs"
I0414 15:20:18.432324       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="93.336¬µs"
I0414 15:20:55.423232       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="284.105¬µs"
I0414 15:21:09.404602       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="118.142¬µs"
I0414 15:24:05.468260       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0414 15:24:29.380864       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="7.501381ms"
I0414 15:24:42.335087       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="453.292¬µs"
I0414 15:25:14.324429       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="88.432¬µs"
I0414 15:25:27.329761       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7df474b884" duration="92.824¬µs"
I0414 15:26:02.314778       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="132.339¬µs"
I0414 15:26:17.300105       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/myapp-deployment-7448ddcf95" duration="159.655¬µs"
I0414 15:26:54.401459       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="74.698227ms"
I0414 15:26:54.422818       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="21.270085ms"
I0414 15:26:54.423158       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="40.712¬µs"
I0414 15:26:54.434046       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="32.31¬µs"
I0414 15:26:54.459538       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="67.021¬µs"
I0414 15:26:56.774435       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="44.004¬µs"
I0414 15:26:57.815979       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="73.107¬µs"
I0414 15:26:58.802780       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="50.505¬µs"
I0414 15:27:14.090741       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="128.242¬µs"
I0414 15:27:28.299943       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="102.227¬µs"
I0414 15:27:41.649024       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="54.817¬µs"
I0414 15:27:53.294524       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="100.931¬µs"
I0414 15:28:22.405831       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="105.842¬µs"
I0414 15:28:35.268474       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="92.829¬µs"
I0414 15:29:42.245462       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="286.979¬µs"
I0414 15:29:42.761533       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="47.313¬µs"
I0414 15:29:53.246195       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-64958bf866" duration="119.034¬µs"
I0414 15:29:54.249960       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/python-jupyter-deployment-7774f97799" duration="43.513¬µs"


==> kube-proxy [6e9a7d2e195e] <==
I0414 15:06:38.311304       1 server_linux.go:66] "Using iptables proxy"
I0414 15:06:44.953735       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0414 15:06:44.954163       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0414 15:06:45.363211       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0414 15:06:45.363390       1 server_linux.go:170] "Using iptables Proxier"
I0414 15:06:45.371926       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0414 15:06:45.437149       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0414 15:06:45.501243       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0414 15:06:45.504585       1 server.go:497] "Version info" version="v1.32.0"
I0414 15:06:45.504663       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0414 15:06:45.591968       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0414 15:06:45.626327       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0414 15:06:45.735261       1 config.go:199] "Starting service config controller"
I0414 15:06:45.736721       1 config.go:105] "Starting endpoint slice config controller"
I0414 15:06:45.743954       1 shared_informer.go:313] Waiting for caches to sync for service config
I0414 15:06:45.744098       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0414 15:06:45.747684       1 config.go:329] "Starting node config controller"
I0414 15:06:45.747748       1 shared_informer.go:313] Waiting for caches to sync for node config
I0414 15:06:45.849041       1 shared_informer.go:320] Caches are synced for node config
I0414 15:06:45.849126       1 shared_informer.go:320] Caches are synced for service config
I0414 15:06:45.849139       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [8a3352176a85] <==
I0414 14:22:04.824270       1 server_linux.go:66] "Using iptables proxy"
I0414 14:22:05.588153       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0414 14:22:05.588526       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0414 14:22:05.665028       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0414 14:22:05.665360       1 server_linux.go:170] "Using iptables Proxier"
I0414 14:22:05.670249       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0414 14:22:05.687011       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0414 14:22:05.701021       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0414 14:22:05.705565       1 server.go:497] "Version info" version="v1.32.0"
I0414 14:22:05.705637       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0414 14:22:05.720100       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0414 14:22:05.731467       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0414 14:22:05.743186       1 config.go:199] "Starting service config controller"
I0414 14:22:05.748689       1 shared_informer.go:313] Waiting for caches to sync for service config
I0414 14:22:05.748844       1 config.go:105] "Starting endpoint slice config controller"
I0414 14:22:05.749047       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0414 14:22:05.759428       1 config.go:329] "Starting node config controller"
I0414 14:22:05.759539       1 shared_informer.go:313] Waiting for caches to sync for node config
I0414 14:22:05.850140       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0414 14:22:05.850264       1 shared_informer.go:320] Caches are synced for service config
I0414 14:22:05.860186       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [0f762f49670d] <==
I0414 14:21:56.926662       1 serving.go:386] Generated self-signed cert in-memory
W0414 14:21:59.613563       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0414 14:21:59.613611       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0414 14:21:59.613626       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0414 14:21:59.613634       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0414 14:21:59.827535       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0414 14:21:59.827577       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0414 14:21:59.842076       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0414 14:21:59.842340       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0414 14:21:59.846225       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0414 14:21:59.848322       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0414 14:21:59.946681       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0414 15:06:10.557972       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0414 15:06:10.563820       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0414 15:06:10.563781       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0414 15:06:10.599480       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [51b35bd6202d] <==
I0414 15:06:42.510564       1 serving.go:386] Generated self-signed cert in-memory
W0414 15:06:44.502532       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0414 15:06:44.502643       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0414 15:06:44.502659       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0414 15:06:44.502686       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0414 15:06:44.838322       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0414 15:06:44.838391       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0414 15:06:44.842326       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0414 15:06:44.842905       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0414 15:06:44.902742       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0414 15:06:44.907246       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0414 15:06:45.005120       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Apr 14 15:27:41 minikube kubelet[1598]: I0414 15:27:41.617735    1598 scope.go:117] "RemoveContainer" containerID="cce98d1549dd7db8fbb9cf4837c82eb023362e1a30d81043240b1ea929ea9ca0"
Apr 14 15:27:41 minikube kubelet[1598]: I0414 15:27:41.618241    1598 scope.go:117] "RemoveContainer" containerID="4e5a2c5a44185882fd4681d0e7d2f39e411f3b10d7ac783e6f1422de56f18938"
Apr 14 15:27:41 minikube kubelet[1598]: E0414 15:27:41.618461    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 40s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:27:44 minikube kubelet[1598]: E0414 15:27:44.263456    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:27:52 minikube kubelet[1598]: E0414 15:27:52.262021    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:27:53 minikube kubelet[1598]: I0414 15:27:53.276926    1598 scope.go:117] "RemoveContainer" containerID="4e5a2c5a44185882fd4681d0e7d2f39e411f3b10d7ac783e6f1422de56f18938"
Apr 14 15:27:53 minikube kubelet[1598]: E0414 15:27:53.277268    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 40s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:27:55 minikube kubelet[1598]: E0414 15:27:55.269732    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:27:56 minikube kubelet[1598]: E0414 15:27:56.264333    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:28:04 minikube kubelet[1598]: E0414 15:28:04.249806    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:28:07 minikube kubelet[1598]: I0414 15:28:07.250492    1598 scope.go:117] "RemoveContainer" containerID="4e5a2c5a44185882fd4681d0e7d2f39e411f3b10d7ac783e6f1422de56f18938"
Apr 14 15:28:07 minikube kubelet[1598]: E0414 15:28:07.250955    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 40s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:28:08 minikube kubelet[1598]: E0414 15:28:08.250390    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:28:11 minikube kubelet[1598]: E0414 15:28:11.250557    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:28:18 minikube kubelet[1598]: E0414 15:28:18.249419    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:28:21 minikube kubelet[1598]: I0414 15:28:21.250161    1598 scope.go:117] "RemoveContainer" containerID="4e5a2c5a44185882fd4681d0e7d2f39e411f3b10d7ac783e6f1422de56f18938"
Apr 14 15:28:22 minikube kubelet[1598]: E0414 15:28:22.250243    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:28:22 minikube kubelet[1598]: I0414 15:28:22.377131    1598 scope.go:117] "RemoveContainer" containerID="4e5a2c5a44185882fd4681d0e7d2f39e411f3b10d7ac783e6f1422de56f18938"
Apr 14 15:28:22 minikube kubelet[1598]: I0414 15:28:22.377808    1598 scope.go:117] "RemoveContainer" containerID="58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257"
Apr 14 15:28:22 minikube kubelet[1598]: E0414 15:28:22.378112    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:28:25 minikube kubelet[1598]: E0414 15:28:25.261025    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:28:31 minikube kubelet[1598]: E0414 15:28:31.245673    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:28:34 minikube kubelet[1598]: E0414 15:28:34.245932    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:28:35 minikube kubelet[1598]: I0414 15:28:35.246539    1598 scope.go:117] "RemoveContainer" containerID="58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257"
Apr 14 15:28:35 minikube kubelet[1598]: E0414 15:28:35.246926    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:28:39 minikube kubelet[1598]: E0414 15:28:39.247189    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:28:45 minikube kubelet[1598]: E0414 15:28:45.246187    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:28:46 minikube kubelet[1598]: E0414 15:28:46.247180    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:28:49 minikube kubelet[1598]: I0414 15:28:49.245306    1598 scope.go:117] "RemoveContainer" containerID="58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257"
Apr 14 15:28:49 minikube kubelet[1598]: E0414 15:28:49.246188    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:28:50 minikube kubelet[1598]: E0414 15:28:50.245861    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:28:59 minikube kubelet[1598]: E0414 15:28:59.247041    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:29:00 minikube kubelet[1598]: I0414 15:29:00.237301    1598 scope.go:117] "RemoveContainer" containerID="58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257"
Apr 14 15:29:00 minikube kubelet[1598]: E0414 15:29:00.237344    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:29:00 minikube kubelet[1598]: E0414 15:29:00.237595    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:29:05 minikube kubelet[1598]: E0414 15:29:05.237662    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:29:12 minikube kubelet[1598]: E0414 15:29:12.237346    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:29:14 minikube kubelet[1598]: I0414 15:29:14.236648    1598 scope.go:117] "RemoveContainer" containerID="58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257"
Apr 14 15:29:14 minikube kubelet[1598]: E0414 15:29:14.237720    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:29:14 minikube kubelet[1598]: E0414 15:29:14.237041    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:29:19 minikube kubelet[1598]: E0414 15:29:19.238739    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:29:24 minikube kubelet[1598]: E0414 15:29:24.242341    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:29:27 minikube kubelet[1598]: I0414 15:29:27.238230    1598 scope.go:117] "RemoveContainer" containerID="58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257"
Apr 14 15:29:27 minikube kubelet[1598]: E0414 15:29:27.240414    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 1m20s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:29:29 minikube kubelet[1598]: E0414 15:29:29.527707    1598 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="python-jupyter-app:latest"
Apr 14 15:29:29 minikube kubelet[1598]: E0414 15:29:29.527906    1598 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="python-jupyter-app:latest"
Apr 14 15:29:29 minikube kubelet[1598]: E0414 15:29:29.528416    1598 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:python-jupyter-app,Image:python-jupyter-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8888,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jvldt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod python-jupyter-deployment-64958bf866-5dq88_default(9761661e-db17-4a2f-8f42-07f7886fd80d): ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Apr 14 15:29:29 minikube kubelet[1598]: E0414 15:29:29.530045    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ErrImagePull: \"Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:29:32 minikube kubelet[1598]: E0414 15:29:32.224155    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:29:37 minikube kubelet[1598]: E0414 15:29:37.226840    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:29:42 minikube kubelet[1598]: E0414 15:29:42.226442    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:29:42 minikube kubelet[1598]: I0414 15:29:42.227013    1598 scope.go:117] "RemoveContainer" containerID="58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257"
Apr 14 15:29:42 minikube kubelet[1598]: I0414 15:29:42.743944    1598 scope.go:117] "RemoveContainer" containerID="58c750c5be4db3b93e0e8448a732f963bce83e2f048b45380838ccbf4d2c9257"
Apr 14 15:29:42 minikube kubelet[1598]: I0414 15:29:42.744264    1598 scope.go:117] "RemoveContainer" containerID="ba4a75831fd89e8df3a1184a0e07a3f77e73f119bd956c4bb1f20147679944f4"
Apr 14 15:29:42 minikube kubelet[1598]: E0414 15:29:42.744546    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"
Apr 14 15:29:43 minikube kubelet[1598]: E0414 15:29:43.234924    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7448ddcf95-tvq2s" podUID="3e70f9c9-ed2a-4130-b284-84893f9c8327"
Apr 14 15:29:52 minikube kubelet[1598]: E0414 15:29:52.225846    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"myapp\" with ImagePullBackOff: \"Back-off pulling image \\\"myapp\\\": ErrImagePull: Error response from daemon: pull access denied for myapp, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/myapp-deployment-7df474b884-4zz57" podUID="4d77693b-0119-4eb5-910d-9e9cd4f69a52"
Apr 14 15:29:53 minikube kubelet[1598]: E0414 15:29:53.225238    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with ImagePullBackOff: \"Back-off pulling image \\\"python-jupyter-app\\\": ErrImagePull: Error response from daemon: pull access denied for python-jupyter-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/python-jupyter-deployment-64958bf866-5dq88" podUID="9761661e-db17-4a2f-8f42-07f7886fd80d"
Apr 14 15:29:54 minikube kubelet[1598]: I0414 15:29:54.224151    1598 scope.go:117] "RemoveContainer" containerID="ba4a75831fd89e8df3a1184a0e07a3f77e73f119bd956c4bb1f20147679944f4"
Apr 14 15:29:54 minikube kubelet[1598]: E0414 15:29:54.224451    1598 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"python-jupyter-app\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=python-jupyter-app pod=python-jupyter-deployment-7774f97799-htmlv_default(45c2c7cd-17b8-433c-95b3-f3bccdf4726d)\"" pod="default/python-jupyter-deployment-7774f97799-htmlv" podUID="45c2c7cd-17b8-433c-95b3-f3bccdf4726d"


==> storage-provisioner [439da701fbeb] <==
I0414 15:06:52.110534       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0414 15:06:52.143536       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0414 15:06:52.144888       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0414 15:07:09.568926       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0414 15:07:09.569186       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_888a211d-eeb0-43ee-b8b0-b60feacde1e0!
I0414 15:07:09.570301       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"888eff84-9a9f-40d5-9eed-b7913a8ce5ab", APIVersion:"v1", ResourceVersion:"4902", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_888a211d-eeb0-43ee-b8b0-b60feacde1e0 became leader
I0414 15:07:09.671932       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_888a211d-eeb0-43ee-b8b0-b60feacde1e0!


==> storage-provisioner [5d498173d4f1] <==
I0414 15:06:36.446198       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0414 15:06:36.539102       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

